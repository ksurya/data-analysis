{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation of Natural Language Rules in Conversational Machine Reading, M. Saeidi et al., EMNLP 2018\n",
    "\n",
    "https://arxiv.org/abs/1809.01494\n",
    "\n",
    "## Step 1/2: Baseline CNN Classifier\n",
    "using BERT embeddings (which isn't part of baseline though!)\n",
    "\n",
    "## Problem\n",
    "\n",
    "Inputs:\n",
    "- Rules (encoded in natural language)\n",
    "- History (question and answers b/w bot and user, that may include disambiguate user's first question)\n",
    "- Question (user's question on rules)\n",
    "\n",
    "Output:\n",
    "- Answer labels\n",
    "  - `Yes` | `No` (bot's answer)\n",
    "  - `More`  (Bot needs more info to answer this question. Step 2/2 of pipeline).\n",
    "  - `Irrelevant` (question is irrelevant to these rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as _tqdm  # required this way by allennlp in jupyter notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.token_indexers import PretrainedBertIndexer\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.data import Instance, fields\n",
    "from allennlp.common.file_utils import cached_path\n",
    "\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding, PretrainedBertEmbedder\n",
    "from allennlp.modules.seq2vec_encoders import PytorchSeq2VecWrapper, CnnEncoder\n",
    "from allennlp.nn.util import sequence_cross_entropy_with_logits, add_sentence_boundary_token_ids, get_text_field_mask\n",
    "\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.predictors import SentenceTaggerPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reader\n",
    "We join History of Follow Up Question and Answers as one text field. Someone interested in creating subsets of data based on history (length, etc) could compute fields needed further down the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class SharcDatasetReader(DatasetReader):\n",
    "    \n",
    "    def __init__(self, token_indexers, tokenizer, lazy=False):\n",
    "        super().__init__(lazy)\n",
    "        self.token_indexers = token_indexers\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def item_to_instance(self, item):\n",
    "        # meta fields\n",
    "        attrs = {\n",
    "            \"utterance_id\": fields.MetadataField(item[\"utterance_id\"]),\n",
    "            \"tree_id\": fields.MetadataField(item[\"tree_id\"]),\n",
    "            \"source_url\": fields.MetadataField(item[\"source_url\"])}\n",
    "\n",
    "        # text fields.\n",
    "        attrs[\"snippet\"] = fields.TextField(\n",
    "            tokens=self.tokenizer(item[\"snippet\"]),\n",
    "            token_indexers=self.token_indexers)\n",
    "        \n",
    "        attrs[\"question\"] = fields.TextField(\n",
    "            tokens=self.tokenizer(item[\"question\"]),\n",
    "            token_indexers=self.token_indexers)\n",
    "        \n",
    "        # \"START\" is a fix for empty cases.\n",
    "        attrs[\"scenario\"] = fields.TextField(\n",
    "            tokens=self.tokenizer(item[\"scenario\"]),\n",
    "            token_indexers=self.token_indexers)\n",
    "        \n",
    "        attrs[\"answer\"] = fields.TextField(\n",
    "            tokens=self.tokenizer(item[\"answer\"]),\n",
    "            token_indexers=self.token_indexers)\n",
    "        \n",
    "        # computed field. answer exists or not.\n",
    "        # labels: yes, no, irrelevant, more\n",
    "        # Note: `label_namespace` can be set to tokens..\n",
    "        attrs[\"answer_exists\"] = fields.LabelField(\n",
    "            label=item[\"answer\"] if item[\"answer\"] in (\"Yes\", \"No\", \"Irrelevant\") else \"More\")\n",
    "        \n",
    "        # history, joined as one string.\n",
    "        history = []\n",
    "        for x in item[\"history\"]:\n",
    "            history.append(x[\"follow_up_question\"])\n",
    "            history.append(x[\"follow_up_answer\"])\n",
    "        attrs[\"history\"] = fields.TextField(\n",
    "            tokens=self.tokenizer(\" \".join(history)),\n",
    "            token_indexers=self.token_indexers)\n",
    "            \n",
    "        return Instance(attrs)\n",
    "        \n",
    "    def _read(self, file_path):\n",
    "        with open(file_path) as fp:\n",
    "            ds = json.load(fp)\n",
    "            for item in ds:\n",
    "                yield self.item_to_instance(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21890it [00:09, 2262.63it/s]\n",
      "2270it [00:00, 3455.81it/s]\n",
      "100%|██████████| 21890/21890 [00:00<00:00, 54891.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Vocabulary with namespaces:  labels, Size: 4 || Non Padded Namespaces: {'*tags', '*labels'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_indexer = PretrainedBertIndexer(\"bert-base-uncased\")\n",
    "\n",
    "def tokenizer(s):\n",
    "    return [Token(t) for t in token_indexer.wordpiece_tokenizer(s)]\n",
    "\n",
    "reader = SharcDatasetReader(token_indexers={\"tokens\": token_indexer}, tokenizer=tokenizer)\n",
    "\n",
    "train_ds = reader.read(cached_path(\"~/sharc/sharc1-official/json/sharc_train.json\"))\n",
    "test_ds = reader.read(cached_path(\"~/sharc/sharc1-official/json/sharc_dev.json\"))\n",
    "\n",
    "vocab = Vocabulary.from_instances(train_ds)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier\n",
    "\n",
    "Section 5.1 describes using samples that have no `scenario` field for training the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4025, 431)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider only instances where `scenario` is null i.e., it only has \"CLS and SEP\"\n",
    "# refer to sec 5.1 in paper.\n",
    "clf_train_ds = [ins for ins in train_ds if len(ins.get(\"scenario\").tokens) == 0]\n",
    "clf_test_ds = [ins for ins in test_ds if len(ins.get(\"scenario\").tokens) == 0]\n",
    "\n",
    "len(clf_train_ds), len(clf_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding = PretrainedBertEmbedder(\"bert-base-uncased\", requires_grad=False)\n",
    "\n",
    "padding_order = [\n",
    "    (\"snippet\", \"num_tokens\"),\n",
    "    (\"scenario\", \"num_tokens\"),\n",
    "    (\"question\", \"num_tokens\"),\n",
    "]\n",
    "\n",
    "iterator = BucketIterator(\n",
    "    batch_size=2,\n",
    "    sorting_keys=padding_order)\n",
    "\n",
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.4489, loss: 153.5865 ||: 100%|██████████| 2013/2013 [01:01<00:00, 33.42it/s]\n",
      "accuracy: 0.4339, loss: 989.1312 ||: 100%|██████████| 216/216 [00:04<00:00, 45.83it/s]\n",
      "accuracy: 0.4557, loss: 376.2320 ||: 100%|██████████| 2013/2013 [00:58<00:00, 33.92it/s]\n",
      "accuracy: 0.4339, loss: 2.7709 ||: 100%|██████████| 216/216 [00:04<00:00, 45.15it/s]\n",
      "accuracy: 0.4492, loss: 2.4610 ||: 100%|██████████| 2013/2013 [00:59<00:00, 34.35it/s]\n",
      "accuracy: 0.5197, loss: 1.0512 ||: 100%|██████████| 216/216 [00:04<00:00, 45.51it/s]\n",
      "accuracy: 0.4778, loss: 0.9944 ||: 100%|██████████| 2013/2013 [00:58<00:00, 35.63it/s]\n",
      "accuracy: 0.5383, loss: 1.0406 ||: 100%|██████████| 216/216 [00:04<00:00, 45.58it/s]\n",
      "accuracy: 0.4333, loss: 451.5980 ||: 100%|██████████| 2013/2013 [01:00<00:00, 32.80it/s]\n",
      "accuracy: 0.5406, loss: 0.8793 ||: 100%|██████████| 216/216 [00:04<00:00, 43.78it/s]\n",
      "accuracy: 0.3752, loss: 1.7987 ||: 100%|██████████| 2013/2013 [01:02<00:00, 30.46it/s]\n",
      "accuracy: 0.3202, loss: 1.4112 ||: 100%|██████████| 216/216 [00:05<00:00, 43.10it/s]\n",
      "accuracy: 0.2812, loss: 1.4033 ||: 100%|██████████| 2013/2013 [01:02<00:00, 32.17it/s]\n",
      "accuracy: 0.2599, loss: 1.3868 ||: 100%|██████████| 216/216 [00:04<00:00, 43.79it/s]\n",
      "accuracy: 0.2716, loss: 1.4023 ||: 100%|██████████| 2013/2013 [01:01<00:00, 32.64it/s]\n",
      "accuracy: 0.3202, loss: 1.3888 ||: 100%|██████████| 216/216 [00:04<00:00, 44.25it/s]\n",
      "accuracy: 0.2785, loss: 1.4048 ||: 100%|██████████| 2013/2013 [01:01<00:00, 32.37it/s]\n",
      "accuracy: 0.2599, loss: 1.4436 ||: 100%|██████████| 216/216 [00:04<00:00, 43.86it/s]\n",
      "accuracy: 0.2817, loss: 1.4038 ||: 100%|██████████| 2013/2013 [01:01<00:00, 33.71it/s]\n",
      "accuracy: 0.2599, loss: 1.4086 ||: 100%|██████████| 216/216 [00:04<00:00, 45.43it/s]\n",
      "accuracy: 0.2937, loss: 1.4048 ||: 100%|██████████| 2013/2013 [00:59<00:00, 33.72it/s]\n",
      "accuracy: 0.2599, loss: 1.4172 ||: 100%|██████████| 216/216 [00:04<00:00, 45.70it/s]\n",
      "accuracy: 0.2855, loss: 1.3999 ||: 100%|██████████| 2013/2013 [00:59<00:00, 34.27it/s]\n",
      "accuracy: 0.3202, loss: 1.3812 ||: 100%|██████████| 216/216 [00:04<00:00, 45.72it/s]\n",
      "accuracy: 0.2852, loss: 1.4035 ||: 100%|██████████| 2013/2013 [00:59<00:00, 34.37it/s]\n",
      "accuracy: 0.2599, loss: 1.4072 ||: 100%|██████████| 216/216 [00:04<00:00, 45.81it/s]\n",
      "accuracy: 0.2860, loss: 1.4044 ||: 100%|██████████| 2013/2013 [00:59<00:00, 34.11it/s]\n",
      "accuracy: 0.2181, loss: 1.4024 ||: 100%|██████████| 216/216 [00:04<00:00, 45.53it/s]\n",
      "accuracy: 0.2760, loss: 1.4065 ||: 100%|██████████| 2013/2013 [00:59<00:00, 33.71it/s]\n",
      "accuracy: 0.3202, loss: 1.3715 ||: 100%|██████████| 216/216 [00:04<00:00, 45.46it/s]\n",
      "accuracy: 0.2850, loss: 1.4013 ||: 100%|██████████| 2013/2013 [00:59<00:00, 32.79it/s]\n",
      "accuracy: 0.2181, loss: 1.3884 ||: 100%|██████████| 216/216 [00:04<00:00, 45.45it/s]\n",
      "accuracy: 0.2855, loss: 1.4035 ||: 100%|██████████| 2013/2013 [00:59<00:00, 33.64it/s]\n",
      "accuracy: 0.2599, loss: 1.4128 ||: 100%|██████████| 216/216 [00:04<00:00, 45.45it/s]\n",
      "accuracy: 0.2907, loss: 1.4025 ||: 100%|██████████| 2013/2013 [00:59<00:00, 35.26it/s]\n",
      "accuracy: 0.3202, loss: 1.4469 ||: 100%|██████████| 216/216 [00:04<00:00, 45.64it/s]\n",
      "accuracy: 0.2793, loss: 1.3993 ||: 100%|██████████| 2013/2013 [00:58<00:00, 34.05it/s]\n",
      "accuracy: 0.3202, loss: 1.4020 ||: 100%|██████████| 216/216 [00:04<00:00, 45.49it/s]\n",
      "accuracy: 0.2882, loss: 1.3988 ||: 100%|██████████| 2013/2013 [00:59<00:00, 34.24it/s]\n",
      "accuracy: 0.3202, loss: 1.3805 ||: 100%|██████████| 216/216 [00:04<00:00, 45.35it/s]\n",
      "accuracy: 0.2760, loss: 1.4036 ||: 100%|██████████| 2013/2013 [00:59<00:00, 33.93it/s]\n",
      "accuracy: 0.2599, loss: 1.3786 ||: 100%|██████████| 216/216 [00:04<00:00, 45.49it/s]\n",
      "accuracy: 0.2730, loss: 1.4023 ||: 100%|██████████| 2013/2013 [00:59<00:00, 33.70it/s]\n",
      "accuracy: 0.2599, loss: 1.4407 ||: 100%|██████████| 216/216 [00:04<00:00, 45.84it/s]\n",
      "accuracy: 0.2795, loss: 1.4030 ||: 100%|██████████| 2013/2013 [00:59<00:00, 33.97it/s]\n",
      "accuracy: 0.2019, loss: 1.4479 ||: 100%|██████████| 216/216 [00:04<00:00, 45.78it/s]\n",
      "accuracy: 0.2817, loss: 1.4065 ||: 100%|██████████| 2013/2013 [00:59<00:00, 35.05it/s]\n",
      "accuracy: 0.2599, loss: 1.4122 ||: 100%|██████████| 216/216 [00:04<00:00, 45.68it/s]\n",
      "accuracy: 0.2837, loss: 1.4010 ||: 100%|██████████| 2013/2013 [00:58<00:00, 34.33it/s]\n",
      "accuracy: 0.2019, loss: 1.4453 ||: 100%|██████████| 216/216 [00:04<00:00, 45.85it/s]\n",
      "accuracy: 0.2899, loss: 1.4002 ||: 100%|██████████| 2013/2013 [00:58<00:00, 34.30it/s]\n",
      "accuracy: 0.3202, loss: 1.3775 ||: 100%|██████████| 216/216 [00:04<00:00, 45.77it/s]\n",
      "accuracy: 0.2835, loss: 1.4015 ||: 100%|██████████| 2013/2013 [00:59<00:00, 34.21it/s]\n",
      "accuracy: 0.3202, loss: 1.3854 ||: 100%|██████████| 216/216 [00:04<00:00, 45.79it/s]\n",
      "accuracy: 0.2867, loss: 1.4028 ||: 100%|██████████| 2013/2013 [00:59<00:00, 34.12it/s]\n",
      "accuracy: 0.2599, loss: 1.3901 ||: 100%|██████████| 216/216 [00:04<00:00, 45.69it/s]\n",
      "accuracy: 0.2753, loss: 1.4052 ||: 100%|██████████| 2013/2013 [00:59<00:00, 33.93it/s]\n",
      "accuracy: 0.2181, loss: 1.3913 ||: 100%|██████████| 216/216 [00:04<00:00, 45.53it/s]\n",
      "accuracy: 0.2827, loss: 1.4024 ||: 100%|██████████| 2013/2013 [00:59<00:00, 33.61it/s]\n",
      "accuracy: 0.3202, loss: 1.3727 ||: 100%|██████████| 216/216 [00:04<00:00, 45.65it/s]\n",
      "accuracy: 0.2862, loss: 1.4017 ||: 100%|██████████| 2013/2013 [00:59<00:00, 33.57it/s]\n",
      "accuracy: 0.2599, loss: 1.4080 ||: 100%|██████████| 216/216 [00:04<00:00, 45.47it/s]\n",
      "accuracy: 0.2827, loss: 1.4068 ||: 100%|██████████| 2013/2013 [00:59<00:00, 33.59it/s]\n",
      "accuracy: 0.3202, loss: 1.3780 ||: 100%|██████████| 216/216 [00:04<00:00, 45.68it/s]\n",
      "accuracy: 0.2862, loss: 1.4027 ||: 100%|██████████| 2013/2013 [00:59<00:00, 34.04it/s]\n",
      "accuracy: 0.2599, loss: 1.3924 ||: 100%|██████████| 216/216 [00:04<00:00, 45.66it/s]\n",
      "accuracy: 0.2701, loss: 1.4092 ||: 100%|██████████| 2013/2013 [01:01<00:00, 34.00it/s]\n",
      "accuracy: 0.3202, loss: 1.3915 ||: 100%|██████████| 216/216 [00:04<00:00, 44.17it/s]\n",
      "accuracy: 0.2860, loss: 1.4026 ||: 100%|██████████| 2013/2013 [01:01<00:00, 33.12it/s]\n",
      "accuracy: 0.2019, loss: 1.4294 ||: 100%|██████████| 216/216 [00:04<00:00, 44.25it/s]\n",
      "accuracy: 0.2765, loss: 1.4018 ||: 100%|██████████| 2013/2013 [01:01<00:00, 33.02it/s]\n",
      "accuracy: 0.2599, loss: 1.4120 ||: 100%|██████████| 216/216 [00:05<00:00, 42.33it/s]\n",
      "accuracy: 0.2817, loss: 1.4041 ||: 100%|██████████| 2013/2013 [01:00<00:00, 33.69it/s]\n",
      "accuracy: 0.3202, loss: 1.4172 ||: 100%|██████████| 216/216 [00:04<00:00, 44.84it/s]\n",
      "accuracy: 0.2770, loss: 1.4058 ||: 100%|██████████| 2013/2013 [01:00<00:00, 33.46it/s]\n",
      "accuracy: 0.3202, loss: 1.3826 ||: 100%|██████████| 216/216 [00:04<00:00, 45.68it/s]\n",
      "accuracy: 0.2917, loss: 1.4034 ||: 100%|██████████| 2013/2013 [00:59<00:00, 34.04it/s]\n",
      "accuracy: 0.3202, loss: 1.3954 ||: 100%|██████████| 216/216 [00:04<00:00, 45.68it/s]\n",
      "accuracy: 0.2850, loss: 1.4021 ||: 100%|██████████| 2013/2013 [00:59<00:00, 33.92it/s]\n",
      "accuracy: 0.2019, loss: 1.4182 ||: 100%|██████████| 216/216 [00:04<00:00, 45.57it/s]\n",
      "accuracy: 0.2778, loss: 1.4014 ||: 100%|██████████| 2013/2013 [00:58<00:00, 34.09it/s]\n",
      "accuracy: 0.3202, loss: 1.3915 ||: 100%|██████████| 216/216 [00:04<00:00, 45.61it/s]\n",
      "accuracy: 0.2884, loss: 1.3991 ||: 100%|██████████| 2013/2013 [00:58<00:00, 34.95it/s]\n",
      "accuracy: 0.2599, loss: 1.4241 ||: 100%|██████████| 216/216 [00:04<00:00, 45.46it/s]\n",
      "accuracy: 0.2939, loss: 1.3992 ||: 100%|██████████| 2013/2013 [00:59<00:00, 34.12it/s]\n",
      "accuracy: 0.2019, loss: 1.4492 ||: 100%|██████████| 216/216 [00:04<00:00, 45.65it/s]\n",
      "accuracy: 0.2820, loss: 1.4000 ||: 100%|██████████| 2013/2013 [00:58<00:00, 35.22it/s]\n",
      "accuracy: 0.3202, loss: 1.4063 ||: 100%|██████████| 216/216 [00:04<00:00, 45.59it/s]\n",
      "accuracy: 0.2820, loss: 1.4024 ||: 100%|██████████| 2013/2013 [00:58<00:00, 34.68it/s]\n",
      "accuracy: 0.3202, loss: 1.4426 ||: 100%|██████████| 216/216 [00:04<00:00, 45.58it/s]\n",
      "accuracy: 0.2805, loss: 1.4063 ||: 100%|██████████| 2013/2013 [00:59<00:00, 34.03it/s]\n",
      "accuracy: 0.3202, loss: 1.3739 ||: 100%|██████████| 216/216 [00:04<00:00, 45.60it/s]\n",
      "accuracy: 0.2860, loss: 1.4022 ||: 100%|██████████| 2013/2013 [00:59<00:00, 33.60it/s]\n",
      "accuracy: 0.2599, loss: 1.3801 ||: 100%|██████████| 216/216 [00:04<00:00, 43.71it/s]\n",
      "accuracy: 0.2688, loss: 1.4039 ||: 100%|██████████| 2013/2013 [01:02<00:00, 32.31it/s]\n",
      "accuracy: 0.3202, loss: 1.3903 ||: 100%|██████████| 216/216 [00:04<00:00, 43.55it/s]\n",
      "accuracy: 0.2778, loss: 1.4045 ||: 100%|██████████| 2013/2013 [01:01<00:00, 32.78it/s]\n",
      "accuracy: 0.3202, loss: 1.3868 ||: 100%|██████████| 216/216 [00:04<00:00, 43.67it/s]\n",
      "accuracy: 0.2760, loss: 1.4056 ||: 100%|██████████| 2013/2013 [01:02<00:00, 32.81it/s]\n",
      "accuracy: 0.3202, loss: 1.3786 ||: 100%|██████████| 216/216 [00:04<00:00, 43.80it/s]\n",
      "accuracy: 0.2867, loss: 1.4041 ||: 100%|██████████| 2013/2013 [01:01<00:00, 32.54it/s]\n",
      "accuracy: 0.3202, loss: 1.3931 ||: 100%|██████████| 216/216 [00:04<00:00, 44.53it/s]\n",
      "accuracy: 0.2758, loss: 1.4062 ||: 100%|██████████| 2013/2013 [00:57<00:00, 35.53it/s]\n",
      "accuracy: 0.2599, loss: 1.4221 ||: 100%|██████████| 216/216 [00:04<00:00, 44.61it/s]\n",
      "accuracy: 0.2795, loss: 1.3991 ||: 100%|██████████| 2013/2013 [00:57<00:00, 34.86it/s]\n",
      "accuracy: 0.3202, loss: 1.3940 ||: 100%|██████████| 216/216 [00:04<00:00, 44.47it/s]\n",
      "accuracy: 0.2842, loss: 1.4030 ||: 100%|██████████| 2013/2013 [01:00<00:00, 33.37it/s]\n",
      "accuracy: 0.3202, loss: 1.4084 ||: 100%|██████████| 216/216 [00:04<00:00, 44.71it/s]\n",
      "accuracy: 0.2966, loss: 1.4002 ||: 100%|██████████| 2013/2013 [01:00<00:00, 32.70it/s]\n",
      "accuracy: 0.2599, loss: 1.4169 ||: 100%|██████████| 216/216 [00:04<00:00, 44.56it/s]\n",
      "accuracy: 0.2857, loss: 1.4003 ||: 100%|██████████| 2013/2013 [01:00<00:00, 33.54it/s]\n",
      "accuracy: 0.3202, loss: 1.3774 ||: 100%|██████████| 216/216 [00:04<00:00, 44.77it/s]\n",
      "accuracy: 0.2773, loss: 1.4069 ||: 100%|██████████| 2013/2013 [01:00<00:00, 34.82it/s]\n",
      "accuracy: 0.3202, loss: 1.3895 ||: 100%|██████████| 216/216 [00:04<00:00, 45.63it/s]\n",
      "accuracy: 0.2795, loss: 1.4048 ||: 100%|██████████| 2013/2013 [01:00<00:00, 33.06it/s]\n",
      "accuracy: 0.2019, loss: 1.4185 ||: 100%|██████████| 216/216 [00:04<00:00, 44.86it/s]\n",
      "accuracy: 0.2795, loss: 1.4045 ||: 100%|██████████| 2013/2013 [01:00<00:00, 33.63it/s]\n",
      "accuracy: 0.2599, loss: 1.3922 ||: 100%|██████████| 216/216 [00:04<00:00, 44.88it/s]\n",
      "accuracy: 0.2825, loss: 1.4067 ||: 100%|██████████| 2013/2013 [01:00<00:00, 33.63it/s]\n",
      "accuracy: 0.2599, loss: 1.3893 ||: 100%|██████████| 216/216 [00:04<00:00, 44.85it/s]\n",
      "accuracy: 0.2870, loss: 1.4026 ||: 100%|██████████| 2013/2013 [01:00<00:00, 33.35it/s]\n",
      "accuracy: 0.3202, loss: 1.3887 ||: 100%|██████████| 216/216 [00:04<00:00, 45.61it/s]\n",
      "accuracy: 0.2889, loss: 1.4018 ||: 100%|██████████| 2013/2013 [00:59<00:00, 33.69it/s]\n",
      "accuracy: 0.3202, loss: 1.4165 ||: 100%|██████████| 216/216 [00:04<00:00, 45.63it/s]\n",
      "accuracy: 0.2738, loss: 1.4025 ||: 100%|██████████| 2013/2013 [01:00<00:00, 32.61it/s]\n",
      "accuracy: 0.2599, loss: 1.3903 ||: 100%|██████████| 216/216 [00:04<00:00, 44.49it/s]\n",
      "accuracy: 0.2822, loss: 1.4018 ||: 100%|██████████| 2013/2013 [01:00<00:00, 33.13it/s]\n",
      "accuracy: 0.3202, loss: 1.3768 ||: 100%|██████████| 216/216 [00:04<00:00, 44.60it/s]\n",
      "accuracy: 0.2832, loss: 1.4043 ||: 100%|██████████| 2013/2013 [01:01<00:00, 32.90it/s]\n",
      "accuracy: 0.3202, loss: 1.3894 ||: 100%|██████████| 216/216 [00:04<00:00, 44.56it/s]\n",
      "accuracy: 0.2810, loss: 1.4017 ||: 100%|██████████| 2013/2013 [01:00<00:00, 33.49it/s]\n",
      "accuracy: 0.3202, loss: 1.3928 ||: 100%|██████████| 216/216 [00:04<00:00, 44.53it/s]\n",
      "accuracy: 0.2825, loss: 1.4016 ||: 100%|██████████| 2013/2013 [01:00<00:00, 33.34it/s]\n",
      "accuracy: 0.3202, loss: 1.3732 ||: 100%|██████████| 216/216 [00:04<00:00, 44.49it/s]\n",
      "accuracy: 0.2815, loss: 1.4039 ||: 100%|██████████| 2013/2013 [01:01<00:00, 33.30it/s]\n",
      "accuracy: 0.2019, loss: 1.4181 ||: 100%|██████████| 216/216 [00:04<00:00, 44.66it/s]\n",
      "accuracy: 0.2807, loss: 1.4042 ||: 100%|██████████| 2013/2013 [01:01<00:00, 32.87it/s]\n",
      "accuracy: 0.2019, loss: 1.4544 ||: 100%|██████████| 216/216 [00:04<00:00, 44.58it/s]\n",
      "accuracy: 0.2747, loss: 1.4049 ||:  37%|███▋      | 748/2013 [00:22<00:37, 33.47it/s]"
     ]
    }
   ],
   "source": [
    "def join_fields(batch, features):\n",
    "    joined = {\"tokens\": [], \"mask\": []}\n",
    "    for f in features:\n",
    "        mask = batch[f][\"mask\"]\n",
    "        tokens = batch[f][\"tokens\"]\n",
    "        # Note: default mask does not inlcude SOS/CLS and EOS/SEP tokens in mask. weird.\n",
    "        if mask.shape != tokens.shape:\n",
    "            del batch[f][\"mask\"]\n",
    "            mask = batch[f][\"mask\"] = get_text_field_mask(batch[f])\n",
    "        joined[\"tokens\"].append(tokens)\n",
    "        joined[\"mask\"].append(mask)\n",
    "    joined[\"tokens\"] = torch.cat(joined[\"tokens\"], dim=-1)\n",
    "    joined[\"mask\"] = torch.cat(joined[\"mask\"], dim=-1)\n",
    "    return joined\n",
    "\n",
    "class YesNoClassifier(Model):\n",
    "    \n",
    "    def __init__(self, embedding, vocab):\n",
    "        super().__init__(vocab)\n",
    "        hidden_size = 100\n",
    "        self.embedding = embedding\n",
    "        self.encoder = CnnEncoder(embedding.get_output_dim(), hidden_size)\n",
    "        self.final = torch.nn.Linear(self.encoder.get_output_dim(), vocab.get_vocab_size(\"labels\"))\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        \n",
    "    def forward(self, *inputs, **kw):\n",
    "        X = join_fields(kw, [\"question\", \"snippet\", \"history\"])\n",
    "        out = self.embedding(X[\"tokens\"])\n",
    "        out = self.encoder(out, X[\"mask\"])\n",
    "        out = self.final(out)\n",
    "        target = \"answer_exists\"\n",
    "        response = {}\n",
    "        if target in kw:\n",
    "            self.accuracy(out, kw[target])\n",
    "            response[\"loss\"] = F.cross_entropy(out, kw[target])\n",
    "        return response\n",
    "    \n",
    "    def get_metrics(self, reset=False):\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}\n",
    "\n",
    "    \n",
    "model = YesNoClassifier(token_embedding, vocab).to(\"cuda\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_dataset=clf_train_ds,\n",
    "    validation_dataset=clf_test_ds,\n",
    "    iterator=iterator,\n",
    "    num_epochs=100,\n",
    "    cuda_device=1,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
